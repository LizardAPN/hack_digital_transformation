{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f11dabb",
   "metadata": {},
   "source": [
    "# Подготовка ноутбука "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f07fbd",
   "metadata": {},
   "source": [
    "## Пробрасываем magic methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97feaebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5324521",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd10fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from data.prepare_data import PrepareData\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings \n",
    "import torch \n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import display\n",
    "import pytesseract\n",
    "import shutil\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "     import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e2582",
   "metadata": {},
   "source": [
    "## Нужные переменные "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ef1558",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path('../../')\n",
    "load_dotenv()\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee36be",
   "metadata": {},
   "source": [
    "# Загрузка датафрейма"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892fd56f",
   "metadata": {},
   "source": [
    "# Тут я просто делю на трейн и тест по своему имно из INC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3307abcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранён CSV: /home/lanmo/hack_digital_transformation/notebooks/all_images.csv (записей: 7707)\n",
      "train: 6936  val: 771\n"
     ]
    }
   ],
   "source": [
    "# pip install pandas pyarrow\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "EXCEL_PATH = r\"/home/lanmo/hack_digital_transformation/data/raw_data/data/metadata/INC/18-001_gin_building_echd_19.08.25.xlsx\"\n",
    "IMAGES_ROOT = r\"/home/lanmo/hack_digital_transformation/data/raw_data/data/metadata/INC/18-001_gin_building_echd_19.08.25\"\n",
    "CSV_ALL   = r\"/home/lanmo/hack_digital_transformation/notebooks/all_images.csv\"\n",
    "CSV_TRAIN = r\"/home/lanmo/hack_digital_transformation/notebooks/train.csv\"\n",
    "CSV_VAL   = r\"/home/lanmo/hack_digital_transformation/notebooks/val.csv\"\n",
    "\n",
    "def excel_to_csv(excel_path, images_root, csv_out,\n",
    "                 filename_col=\"Имя файла\",\n",
    "                 lat_col=\"latitude\", lon_col=\"longitude\",\n",
    "                 n_subset=None, val_size=0.1, seed=42):\n",
    "    # читаем как строки, чтобы не сломать числа с запятой\n",
    "    df = pd.read_excel(excel_path, dtype=str)\n",
    "\n",
    "    # переименуем/нормализуем нужные колонки\n",
    "    rename_map = {\n",
    "        filename_col: \"path\",\n",
    "        lat_col: \"lat\",\n",
    "        lon_col: \"lon\",\n",
    "    }\n",
    "    for k in [filename_col, lat_col, lon_col]:\n",
    "        if k not in df.columns:\n",
    "            raise ValueError(f\"В Excel нет обязательной колонки: {k}\")\n",
    "\n",
    "    df = df.rename(columns=rename_map)[[\"path\", \"lat\", \"lon\"]]\n",
    "\n",
    "    # абсолютные пути к картинкам\n",
    "    df[\"path\"] = df[\"path\"].apply(lambda x: os.path.join(images_root, str(x)))\n",
    "\n",
    "    # lat/lon: запятая -> точка -> float\n",
    "    df[\"lat\"] = df[\"lat\"].str.replace(\",\", \".\", regex=False).astype(float)\n",
    "    df[\"lon\"] = df[\"lon\"].str.replace(\",\", \".\", regex=False).astype(float)\n",
    "\n",
    "    # оставляем только существующие файлы\n",
    "    df = df[df[\"path\"].apply(os.path.exists)].reset_index(drop=True)\n",
    "\n",
    "    # опционально берём подмножество\n",
    "    if n_subset is not None and n_subset < len(df):\n",
    "        df = df.sample(n_subset, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # сохраняем общий CSV\n",
    "    df.to_csv(csv_out, index=False)\n",
    "    print(f\"Сохранён CSV: {csv_out} (записей: {len(df)})\")\n",
    "\n",
    "    # train/val\n",
    "    train_df, val_df = train_test_split(df, test_size=val_size, random_state=seed)\n",
    "    train_df.to_csv(CSV_TRAIN, index=False)\n",
    "    val_df.to_csv(CSV_VAL, index=False)\n",
    "    print(f\"train: {len(train_df)}  val: {len(val_df)}\")\n",
    "\n",
    "# пример вызова: возьмём 5000 строк для прогона\n",
    "excel_to_csv(EXCEL_PATH, IMAGES_ROOT, CSV_ALL, n_subset=30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fc6ee2",
   "metadata": {},
   "source": [
    "# Очень годно детектит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d1c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готово: найдено 3 боксов. Сохранено в street_houses.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lanmo/hack_digital_transformation/.venv/lib/python3.12/site-packages/transformers/models/owlv2/processing_owlv2.py:201: FutureWarning: `post_process_object_detection` method is deprecated for OwlVitProcessor and will be removed in v5. Use `post_process_grounded_object_detection` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[4.1678829e+01, 6.5760934e-01, 7.0213861e+02, 4.8493274e+02],\n",
       "        [7.2962201e+02, 3.9257327e+02, 8.5870062e+02, 4.8749780e+02],\n",
       "        [1.1606184e+00, 2.3607016e-02, 7.7130653e+01, 4.6877594e+02]],\n",
       "       dtype=float32),\n",
       " array([0.7730474 , 0.42973122, 0.39342672], dtype=float32))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection  \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\").to(device).eval()\n",
    "\n",
    "texts = [[\"house\", \"building\", \"apartment building\", \"home\"]]\n",
    "\n",
    "def detect_houses(image_path, score_thresh=0.25, iou_thresh=0.5, out_path=\"pred.jpg\"):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)  # (H, W)\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs=outputs,\n",
    "        threshold=score_thresh,\n",
    "        target_sizes=target_sizes\n",
    "    )[0]\n",
    "\n",
    "    boxes = results[\"boxes\"].cpu()    \n",
    "    scores = results[\"scores\"].cpu()\n",
    "    labels = results[\"labels\"].cpu()  \n",
    "\n",
    "    keep = torch.ops.torchvision.nms(boxes, scores, iou_thresh)\n",
    "    boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
    "\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"DejaVuSans.ttf\", 18)\n",
    "    except:\n",
    "        font = None\n",
    "\n",
    "    for box, score, lab in zip(boxes, scores, labels):\n",
    "        x1, y1, x2, y2 = map(float, box.tolist())\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=(0, 255, 0), width=3)\n",
    "        name = texts[0][int(lab)]\n",
    "        txt = f\"{name}: {score:.2f}\"\n",
    "        tw, th = draw.textlength(txt, font=font), 18\n",
    "        draw.rectangle([x1, y1 - th - 4, x1 + tw + 6, y1], fill=(0, 255, 0))\n",
    "        draw.text((x1 + 3, y1 - th - 2), txt, fill=(0, 0, 0), font=font)\n",
    "\n",
    "    image.save(out_path)\n",
    "    print(f\"Готово: найдено {len(boxes)} боксов. Сохранено в {out_path}\")\n",
    "    return boxes.numpy(), scores.numpy()\n",
    "\n",
    "detect_houses(тут ссылка, score_thresh=0.25, iou_thresh=0.5, out_path=\"street_houses.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e131978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train enc: load+preprocess: 100%|██████████| 6936/6936 [1:39:24<00:00,  1.16img/s]  \n",
      "Test enc: load+preprocess: 100%|██████████| 771/771 [11:07<00:00,  1.15img/s]\n",
      "FAISS search: 100%|██████████| 2/2 [00:00<00:00,  8.59batch/s]\n",
      "Assembling results: 100%|██████████| 771/771 [00:00<00:00, 67962.85query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Готово! Результаты сохранены в knn_results.csv\n",
      "\n",
      "=== Metrics ===\n",
      "  Coverage_with_coords_%: 100.00\n",
      "        Geo_MRR@10_≤100m: 0.1944\n",
      "         Geo_MRR@10_≤25m: 0.1768\n",
      "         Geo_MRR@10_≤50m: 0.1845\n",
      "         Geo_MRR@1_≤100m: 0.1751\n",
      "          Geo_MRR@1_≤25m: 0.1634\n",
      "          Geo_MRR@1_≤50m: 0.1699\n",
      "         Geo_MRR@5_≤100m: 0.1944\n",
      "          Geo_MRR@5_≤25m: 0.1768\n",
      "          Geo_MRR@5_≤50m: 0.1845\n",
      "     Geo_Recall@10_≤100m: 0.2283\n",
      "      Geo_Recall@10_≤25m: 0.2023\n",
      "      Geo_Recall@10_≤50m: 0.2114\n",
      "      Geo_Recall@1_≤100m: 0.1751\n",
      "       Geo_Recall@1_≤25m: 0.1634\n",
      "       Geo_Recall@1_≤50m: 0.1699\n",
      "      Geo_Recall@5_≤100m: 0.2283\n",
      "       Geo_Recall@5_≤25m: 0.2023\n",
      "       Geo_Recall@5_≤50m: 0.2114\n",
      "    Top1_Distance_mean_m: 12713.91\n",
      "  Top1_Distance_median_m: 11791.44\n",
      "     Top1_Distance_std_m: 10583.09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import faiss\n",
    "from geoclip import ImageEncoder\n",
    "\n",
    "\n",
    "# ------------------------- Data utils -------------------------\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    paths: List[str]\n",
    "    lat: Optional[np.ndarray] = None\n",
    "    lon: Optional[np.ndarray] = None\n",
    "    label: Optional[np.ndarray] = None\n",
    "\n",
    "\n",
    "def read_csv_dataset(csv_path: str,\n",
    "                     path_col: str = \"path\",\n",
    "                     lat_col: str = \"lat\",\n",
    "                     lon_col: str = \"lon\",\n",
    "                     label_col: str = \"label\") -> Dataset:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if path_col not in df.columns:\n",
    "        raise ValueError(f\"CSV {csv_path} не содержит столбец '{path_col}'.\")\n",
    "    paths = df[path_col].astype(str).tolist()\n",
    "    lat = df[lat_col].to_numpy() if lat_col in df.columns else None\n",
    "    lon = df[lon_col].to_numpy() if lon_col in df.columns else None\n",
    "    label = df[label_col].to_numpy() if label_col in df.columns else None\n",
    "    return Dataset(paths=paths, lat=lat, lon=lon, label=label)\n",
    "\n",
    "\n",
    "def load_image(path: str) -> Image.Image:\n",
    "    return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# ------------------------- Encoding -------------------------\n",
    "\n",
    "def get_model_device(model: torch.nn.Module) -> torch.device:\n",
    "    try:\n",
    "        return next(model.parameters()).device\n",
    "    except Exception:\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def encode_images(\n",
    "    image_encoder: ImageEncoder,\n",
    "    image_paths: List[str],\n",
    "    batch_size: int = 64,\n",
    "    desc: str = \"Encoding\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Кодирует список путей в эмбеддинги, используя строго API geoclip:\n",
    "      - image_encoder.preprocess_image(PIL.Image) -> torch.Tensor [C,H,W] или [1,C,H,W]\n",
    "      - image_encoder(tensor[B,C,H,W]) -> torch.Tensor [B,D]\n",
    "    Работает батчами и показывает прогресс через tqdm.\n",
    "    \"\"\"\n",
    "    device = get_model_device(image_encoder)\n",
    "    image_encoder.eval()  # на всякий случай\n",
    "    embs_chunks: List[np.ndarray] = []\n",
    "\n",
    "    cur_batch: List[torch.Tensor] = []\n",
    "    for path in tqdm(image_paths, desc=f\"{desc}: load+preprocess\", unit=\"img\"):\n",
    "        img = load_image(path)\n",
    "        t = image_encoder.preprocess_image(img)  # -> torch.Tensor\n",
    "        if not isinstance(t, torch.Tensor):\n",
    "            t = torch.as_tensor(t)\n",
    "\n",
    "        if t.ndim == 3:  # [C,H,W] -> [1,C,H,W]\n",
    "            t = t.unsqueeze(0)\n",
    "\n",
    "        cur_batch.append(t)\n",
    "\n",
    "        if len(cur_batch) >= batch_size:\n",
    "            batch = torch.cat(cur_batch, dim=0).to(device)  # [B,C,H,W]\n",
    "            with torch.no_grad():\n",
    "                out = image_encoder(batch)                  # [B,D]\n",
    "            out = out.detach().cpu().numpy().astype(\"float32\", copy=False)\n",
    "            embs_chunks.append(out)\n",
    "            cur_batch.clear()\n",
    "\n",
    "    # хвост\n",
    "    if cur_batch:\n",
    "        batch = torch.cat(cur_batch, dim=0).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = image_encoder(batch)\n",
    "        out = out.detach().cpu().numpy().astype(\"float32\", copy=False)\n",
    "        embs_chunks.append(out)\n",
    "\n",
    "    return np.vstack(embs_chunks)\n",
    "\n",
    "\n",
    "def l2_normalize(x: np.ndarray, axis: int = 1, eps: float = 1e-12) -> np.ndarray:\n",
    "    norm = np.linalg.norm(x, ord=2, axis=axis, keepdims=True)\n",
    "    return x / np.maximum(norm, eps)\n",
    "\n",
    "\n",
    "# ------------------------- FAISS -------------------------\n",
    "\n",
    "def build_faiss_ip_index(vectors: np.ndarray) -> faiss.Index:\n",
    "    \"\"\"\n",
    "    Индекс для косинусной близости:\n",
    "    при L2-нормировке векторов косинусная схожесть == скалярному произведению.\n",
    "    \"\"\"\n",
    "    d = vectors.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(vectors)\n",
    "    return index\n",
    "\n",
    "\n",
    "def search_knn_batched(\n",
    "    index: faiss.Index,\n",
    "    query_vectors: np.ndarray,\n",
    "    k: int,\n",
    "    search_batch_size: int = 4096,\n",
    "    desc: str = \"FAISS search\",\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    all_scores, all_indices = [], []\n",
    "    for i in tqdm(range(0, len(query_vectors), search_batch_size), desc=desc, unit=\"batch\"):\n",
    "        q = query_vectors[i:i+search_batch_size]\n",
    "        s, idx = index.search(q, k)\n",
    "        all_scores.append(s)\n",
    "        all_indices.append(idx)\n",
    "    return np.vstack(all_scores), np.vstack(all_indices)\n",
    "\n",
    "\n",
    "# ------------------------- Metrics -------------------------\n",
    "\n",
    "def haversine_m(c1: Tuple[float, float], c2: Tuple[float, float]) -> float:\n",
    "    \"\"\"Расстояние между двумя точками (lat, lon) в метрах.\"\"\"\n",
    "    R = 6371000.0\n",
    "    lat1, lon1 = np.radians(c1[0]), np.radians(c1[1])\n",
    "    lat2, lon2 = np.radians(c2[0]), np.radians(c2[1])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return float(2 * R * np.arcsin(np.sqrt(a)))\n",
    "\n",
    "\n",
    "def compute_geo_metrics(\n",
    "    results_df: pd.DataFrame,\n",
    "    ks: List[int] = [1, 5, 10],\n",
    "    radii_m: List[float] = [25.0, 50.0, 100.0],\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Считает Geo-Recall@K и Geo-MRR@K по спискам K и радиусов; также среднюю топ-1 дистанцию.\"\"\"\n",
    "    metrics: Dict[str, float] = {}\n",
    "\n",
    "    # Проверим, есть ли координаты\n",
    "    if not {\"test_lat\", \"test_lon\", \"train_lat\", \"train_lon\"}.issubset(results_df.columns):\n",
    "        return metrics  # нет координат — нет гео-метрик\n",
    "\n",
    "    # Средняя дистанция до top-1 соседа\n",
    "    top1_rows = results_df[results_df[\"neighbor_rank\"] == 1].copy()\n",
    "    valid_top1 = top1_rows.dropna(subset=[\"test_lat\", \"test_lon\", \"train_lat\", \"train_lon\"])\n",
    "    if len(valid_top1) > 0:\n",
    "        dists = [\n",
    "            haversine_m((r[\"test_lat\"], r[\"test_lon\"]), (r[\"train_lat\"], r[\"train_lon\"]))\n",
    "            for _, r in valid_top1.iterrows()\n",
    "        ]\n",
    "        metrics[\"Top1_Distance_mean_m\"] = float(np.mean(dists))\n",
    "        metrics[\"Top1_Distance_median_m\"] = float(np.median(dists))\n",
    "        metrics[\"Top1_Distance_std_m\"] = float(np.std(dists))\n",
    "        metrics[\"Coverage_with_coords_%\"] = 100.0 * len(valid_top1) / top1_rows.shape[0]\n",
    "\n",
    "    # Recall@K и MRR@K для разных радиусов\n",
    "    grouped = results_df.groupby(\"test_id\")\n",
    "\n",
    "    for rad in radii_m:\n",
    "        for K in ks:\n",
    "            hits = []\n",
    "            rr_list = []\n",
    "            for test_id, group in grouped:\n",
    "                groupK = group.nsmallest(K, \"neighbor_rank\")\n",
    "                groupK = groupK.dropna(subset=[\"test_lat\", \"test_lon\", \"train_lat\", \"train_lon\"])\n",
    "                if groupK.empty:\n",
    "                    hits.append(False)\n",
    "                    rr_list.append(0.0)\n",
    "                    continue\n",
    "\n",
    "                # Найдём ранг первого соседа в радиусе\n",
    "                found_rank = None\n",
    "                for _, row in groupK.iterrows():\n",
    "                    d = haversine_m((row[\"test_lat\"], row[\"test_lon\"]),\n",
    "                                    (row[\"train_lat\"], row[\"train_lon\"]))\n",
    "                    if d <= rad:\n",
    "                        found_rank = int(row[\"neighbor_rank\"])\n",
    "                        break\n",
    "\n",
    "                if found_rank is None:\n",
    "                    hits.append(False)\n",
    "                    rr_list.append(0.0)\n",
    "                else:\n",
    "                    hits.append(True)\n",
    "                    rr_list.append(1.0 / found_rank)\n",
    "\n",
    "            metrics[f\"Geo_Recall@{K}_≤{int(rad)}m\"] = float(np.mean(hits)) if hits else np.nan\n",
    "            metrics[f\"Geo_MRR@{K}_≤{int(rad)}m\"] = float(np.mean(rr_list)) if rr_list else np.nan\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_label_recall_at_k(\n",
    "    results_df: pd.DataFrame,\n",
    "    ks: List[int] = [1, 5, 10],\n",
    "    label_col: str = \"label\",\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Если в данных есть 'label', считаем Recall@K по лейблам (правильный — сосед с тем же label).\"\"\"\n",
    "    metrics: Dict[str, float] = {}\n",
    "    need_cols = {\"test_id\", \"neighbor_rank\", label_col}\n",
    "    if not need_cols.issubset(results_df.columns):\n",
    "        return metrics\n",
    "    grouped = results_df.groupby(\"test_id\")\n",
    "    for K in ks:\n",
    "        hits = []\n",
    "        for test_id, group in grouped:\n",
    "            # test label предполагаем одинаковый в пределах группы\n",
    "            if group[label_col].isna().all():\n",
    "                hits.append(False)\n",
    "                continue\n",
    "            test_label = group[label_col].iloc[0]\n",
    "            topK = group.nsmallest(K, \"neighbor_rank\")\n",
    "            ok = (topK[label_col] == test_label).any()\n",
    "            hits.append(bool(ok))\n",
    "        metrics[f\"Label_Recall@{K}\"] = float(np.mean(hits)) if hits else np.nan\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ------------------------- Pipeline -------------------------\n",
    "\n",
    "def knn_on_train_test(\n",
    "    train_csv: str,\n",
    "    test_csv: str,\n",
    "    k: int = 5,\n",
    "    batch_size: int = 64,\n",
    "    search_batch_size: int = 4096,\n",
    "    normalize: bool = True,\n",
    "    save_index_path: Optional[str] = None,\n",
    "    save_embeddings_dir: Optional[str] = None,\n",
    "    path_col: str = \"path\",\n",
    "    lat_col: str = \"lat\",\n",
    "    lon_col: str = \"lon\",\n",
    "    label_col: str = \"label\",\n",
    "    metric_ks: List[int] = [1, 5, 10],\n",
    "    metric_radii_m: List[float] = [25.0, 50.0, 100.0],\n",
    ") -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
    "    # 1) Данные\n",
    "    train_ds = read_csv_dataset(train_csv, path_col, lat_col, lon_col, label_col)\n",
    "    test_ds  = read_csv_dataset(test_csv,  path_col, lat_col, lon_col, label_col)\n",
    "\n",
    "    # 2) Энкодер\n",
    "    image_encoder = ImageEncoder()\n",
    "\n",
    "    # 3) Эмбеддинги (с прогрессом)\n",
    "    train_embs = encode_images(image_encoder, train_ds.paths, batch_size=batch_size, desc=\"Train enc\")\n",
    "    test_embs  = encode_images(image_encoder, test_ds.paths,  batch_size=batch_size, desc=\"Test enc\")\n",
    "\n",
    "    # 4) Нормализация (для косинусной близости через IP)\n",
    "    if normalize:\n",
    "        train_embs = l2_normalize(train_embs)\n",
    "        test_embs  = l2_normalize(test_embs)\n",
    "\n",
    "    # (опционально) Сохранить эмбеддинги\n",
    "    if save_embeddings_dir:\n",
    "        os.makedirs(save_embeddings_dir, exist_ok=True)\n",
    "        np.save(os.path.join(save_embeddings_dir, \"train_embs.npy\"), train_embs)\n",
    "        np.save(os.path.join(save_embeddings_dir, \"test_embs.npy\"),  test_embs)\n",
    "\n",
    "    # 5) Индекс FAISS\n",
    "    index = build_faiss_ip_index(train_embs)\n",
    "    if save_index_path:\n",
    "        faiss.write_index(index, save_index_path)\n",
    "\n",
    "    # 6) Поиск kNN (с прогрессом)\n",
    "    scores, indices = search_knn_batched(index, test_embs, k=k, search_batch_size=search_batch_size)\n",
    "\n",
    "    # 7) Сбор результатов (с прогрессом)\n",
    "    rows = []\n",
    "    for i in tqdm(range(len(test_ds.paths)), desc=\"Assembling results\", unit=\"query\"):\n",
    "        test_path = test_ds.paths[i]\n",
    "        test_lat  = None if test_ds.lat is None else float(test_ds.lat[i])\n",
    "        test_lon  = None if test_ds.lon is None else float(test_ds.lon[i])\n",
    "        test_lbl  = None if test_ds.label is None else test_ds.label[i]\n",
    "\n",
    "        for j in range(k):\n",
    "            train_idx = int(indices[i, j])\n",
    "            row = {\n",
    "                \"test_id\": i,\n",
    "                \"test_path\": test_path,\n",
    "                \"neighbor_rank\": j + 1,\n",
    "                \"train_id\": train_idx,\n",
    "                \"train_path\": train_ds.paths[train_idx],\n",
    "                \"similarity\": float(scores[i, j]),\n",
    "                \"test_lat\":  test_lat,\n",
    "                \"test_lon\":  test_lon,\n",
    "                \"train_lat\": None if train_ds.lat is None else float(train_ds.lat[train_idx]),\n",
    "                \"train_lon\": None if train_ds.lon is None else float(train_ds.lon[train_idx]),\n",
    "            }\n",
    "            # Прокинем label, если есть\n",
    "            if test_ds.label is not None:\n",
    "                row[\"label\"] = test_lbl\n",
    "                if train_ds.label is not None:\n",
    "                    row[\"train_label\"] = train_ds.label[train_idx]\n",
    "            rows.append(row)\n",
    "\n",
    "    result_df = pd.DataFrame(rows)\n",
    "\n",
    "    # 8) Метрики\n",
    "    metrics = {}\n",
    "    metrics.update(compute_geo_metrics(result_df, ks=metric_ks, radii_m=metric_radii_m))\n",
    "    # Label-based только если есть колонка label и (опц.) train_label\n",
    "    if \"label\" in result_df.columns:\n",
    "        # Для совместимости: если нет train_label, используем label из train при сборке выше\n",
    "        if \"train_label\" in result_df.columns:\n",
    "            # подменим столбец для проверки совпадения\n",
    "            same = (result_df[\"train_label\"] == result_df[\"label\"])\n",
    "            result_df = result_df.assign(_same_label=same)\n",
    "        metrics.update(compute_label_recall_at_k(result_df, ks=metric_ks, label_col=\"label\"))\n",
    "\n",
    "    return result_df, metrics\n",
    "\n",
    "\n",
    "# ------------------------- Entry point -------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Укажите свои CSV\n",
    "    TRAIN_CSV = r\"/home/lanmo/hack_digital_transformation/notebooks/train.csv\"   # CSV со столбцами path,lat,lon,[label]\n",
    "    TEST_CSV  = r\"/home/lanmo/hack_digital_transformation/notebooks/val.csv\"    # CSV со столбцами path,lat,lon,[label]\n",
    "\n",
    "    # Гиперпараметры\n",
    "    K = 5\n",
    "    BATCH = 128\n",
    "    SEARCH_BATCH = 512  # уменьшите, если не хватает RAM\n",
    "\n",
    "    # Наборы K и радиусов (м) для метрик\n",
    "    METRIC_KS = [1, 5, 10]\n",
    "    METRIC_RADII = [25.0, 50.0, 100.0]\n",
    "\n",
    "    results_df, metrics = knn_on_train_test(\n",
    "        train_csv=TRAIN_CSV,\n",
    "        test_csv=TEST_CSV,\n",
    "        k=K,\n",
    "        batch_size=BATCH,\n",
    "        search_batch_size=SEARCH_BATCH,\n",
    "        normalize=True,\n",
    "        save_index_path=\"faiss_index.ip\",        # или None\n",
    "        save_embeddings_dir=\"embeddings_cache\",  # или None\n",
    "        metric_ks=METRIC_KS,\n",
    "        metric_radii_m=METRIC_RADII,\n",
    "    )\n",
    "\n",
    "    out_csv = \"knn_results.csv\"\n",
    "    results_df.to_csv(out_csv, index=False)\n",
    "    print(f\"\\nГотово! Результаты сохранены в {out_csv}\")\n",
    "\n",
    "    # Печать метрик красиво\n",
    "    if metrics:\n",
    "        print(\"\\n=== Metrics ===\")\n",
    "        # Сортируем ключи для аккуратного вывода\n",
    "        for k in sorted(metrics.keys()):\n",
    "            v = metrics[k]\n",
    "            if \"Recall\" in k or \"MRR\" in k:\n",
    "                print(f\"{k:>24}: {v:.4f}\")\n",
    "            elif \"Distance\" in k or \"Coverage\" in k:\n",
    "                print(f\"{k:>24}: {v:.2f}\")\n",
    "            else:\n",
    "                try:\n",
    "                    print(f\"{k:>24}: {float(v):.4f}\")\n",
    "                except Exception:\n",
    "                    print(f\"{k:>24}: {v}\")\n",
    "    else:\n",
    "        print(\"\\n(Координаты/лейблы не найдены — метрики не посчитаны.)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack_digital_transformation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
